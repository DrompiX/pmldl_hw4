{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWH5liDEcHJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCj6sQxrfE-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C9VkvfQTXrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import click\n",
        "from pathlib import Path\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhcVoJbdE4Ph",
        "colab_type": "code",
        "outputId": "e6066e85-c2a8-4490-fba4-d877b4b33878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8SRCVobFVIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r '/content/drive/My Drive/data_enru' 'data_enru'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOECX_xdMIyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r '/content/drive/My Drive/data_orig' 'data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss3_9XYvKraK",
        "colab_type": "code",
        "outputId": "1871c43a-9a51-4b68-ae39-b87400a54d63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.6/dist-packages (0.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlL9oErLUTXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBYZNJWt_Q1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ParallelLanguageDataset(Dataset):\n",
        "    def __init__(self, data_path_1, data_path_2, num_tokens, max_seq_length):\n",
        "        self.num_tokens = num_tokens\n",
        "        self.data_1, self.data_2, self.data_lengths = load_data(data_path_1, data_path_2, max_seq_length)\n",
        "\n",
        "        self.batches = gen_batches(num_tokens, self.data_lengths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, src_mask = getitem(idx, self.data_1, self.batches, True)\n",
        "        tgt, tgt_mask = getitem(idx, self.data_2, self.batches, False)\n",
        "\n",
        "        return src, src_mask, tgt, tgt_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def shuffle_batches(self):\n",
        "        self.batches = gen_batches(self.num_tokens, self.data_lengths)\n",
        "\n",
        "\n",
        "def gen_batches(num_tokens, data_lengths):\n",
        "    # Shuffle all the indices\n",
        "    for k, v in data_lengths.items():\n",
        "        random.shuffle(v)\n",
        "\n",
        "    batches = []\n",
        "    prev_tokens_in_batch = 1e10\n",
        "    for k in sorted(data_lengths):\n",
        "        v = data_lengths[k]\n",
        "        total_tokens = (k[0] + k[1]) * len(v)\n",
        "\n",
        "        while total_tokens > 0:\n",
        "            tokens_in_batch = min(total_tokens, num_tokens) - min(total_tokens, num_tokens) % (k[0] + k[1])\n",
        "            sentences_in_batch = tokens_in_batch // (k[0] + k[1])\n",
        "\n",
        "            # Combine with previous batch?\n",
        "            if tokens_in_batch + prev_tokens_in_batch <= num_tokens:\n",
        "                batches[-1].extend(v[:sentences_in_batch])\n",
        "                prev_tokens_in_batch += tokens_in_batch\n",
        "            else:\n",
        "                batches.append(v[:sentences_in_batch])\n",
        "                prev_tokens_in_batch = tokens_in_batch\n",
        "            v = v[sentences_in_batch:]\n",
        "\n",
        "            total_tokens = (k[0] + k[1]) * len(v)\n",
        "    return batches\n",
        "\n",
        "\n",
        "def load_data(data_path_1, data_path_2, max_seq_length):\n",
        "    with open(data_path_1, 'rb') as f:\n",
        "        data_1 = pickle.load(f)\n",
        "    with open(data_path_2, 'rb') as f:\n",
        "        data_2 = pickle.load(f)\n",
        "\n",
        "    data_lengths = {}\n",
        "    for i, (str_1, str_2) in enumerate(zip(data_1, data_2)):\n",
        "        if 0 < len(str_1) <= max_seq_length and 0 < len(str_2) <= max_seq_length - 2:\n",
        "            if (len(str_1), len(str_2)) in data_lengths:\n",
        "                data_lengths[(len(str_1), len(str_2))].append(i)\n",
        "            else:\n",
        "                data_lengths[(len(str_1), len(str_2))] = [i]\n",
        "    return data_1, data_2, data_lengths\n",
        "\n",
        "\n",
        "def getitem(idx, data, batches, src):\n",
        "    sentence_indices = batches[idx]\n",
        "    if src:\n",
        "        batch = [data[i] for i in sentence_indices]\n",
        "    else:\n",
        "        batch = [[2] + data[i] + [3] for i in sentence_indices]\n",
        "\n",
        "    seq_length = 0\n",
        "    for sentence in batch:\n",
        "        if len(sentence) > seq_length:\n",
        "            seq_length = len(sentence)\n",
        "\n",
        "    masks = []\n",
        "    for i, sentence in enumerate(batch):\n",
        "        masks.append([False for _ in range(len(sentence))] + [True for _ in range(seq_length - len(sentence))])\n",
        "        batch[i] = sentence + [0 for _ in range(seq_length - len(sentence))]\n",
        "\n",
        "    return np.array(batch), np.array(masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv7YH4ZwohDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# From https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Optim.py\n",
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients by the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power(self.n_current_steps, -0.5),\n",
        "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBPlpNAySGe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class LanguageTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length, pos_dropout, trans_dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed_src = nn.Embedding(vocab_size, d_model)\n",
        "        self.embed_tgt = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, pos_dropout, max_seq_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "        src = rearrange(src, 'n s -> s n')\n",
        "        tgt = rearrange(tgt, 'n t -> t n')\n",
        "        src = self.pos_enc(self.embed_src(src) * math.sqrt(self.d_model))\n",
        "        tgt = self.pos_enc(self.embed_tgt(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n",
        "                                  tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "        output = rearrange(output, 't n e -> n t e')\n",
        "        return self.fc(output)\n",
        "\n",
        "\n",
        "# Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IPsf5_5SRCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # project_path = str(Path(__file__).resolve().parents[0])\n",
        "\n",
        "    train_dataset = ParallelLanguageDataset('data/processed/en/train.pkl',\n",
        "                                            'data/processed/fr/train.pkl',\n",
        "                                            num_tokens, max_seq_length)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valid_dataset = ParallelLanguageDataset('data/processed/en/val.pkl',\n",
        "                                            'data/processed/fr/val.pkl',\n",
        "                                            num_tokens, max_seq_length)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    model = LanguageTransformer(vocab_size, d_model, nhead, num_encoder_layers,\n",
        "                                num_decoder_layers, dim_feedforward, max_seq_length,\n",
        "                                pos_dropout, trans_dropout).to(device)\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_normal_(p)\n",
        "\n",
        "    optim = ScheduledOptim(\n",
        "        Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "        d_model, n_warmup_steps)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    train_losses, val_losses = train(train_loader, valid_loader, model, optim, criterion, num_epochs)\n",
        "\n",
        "\n",
        "def train(train_loader, valid_loader, model, optim, criterion, num_epochs):\n",
        "    print_every = 500\n",
        "    model.train()\n",
        "\n",
        "    lowest_val = 1e9\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    total_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm(total=print_every, leave=False)\n",
        "        total_loss = 0\n",
        "\n",
        "        train_loader.dataset.shuffle_batches()\n",
        "        for step, (src, src_key_padding_mask, tgt, tgt_key_padding_mask) in enumerate(iter(train_loader)):\n",
        "            total_step += 1\n",
        "\n",
        "            src, src_key_padding_mask = src[0].to(device), src_key_padding_mask[0].to(device)\n",
        "            tgt, tgt_key_padding_mask = tgt[0].to(device), tgt_key_padding_mask[0].to(device)\n",
        "\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            tgt_inp, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "            tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
        "            loss = criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step_and_update_lr()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            train_losses.append((step, loss.item()))\n",
        "            pbar.update(1)\n",
        "            if step % print_every == print_every - 1:\n",
        "                pbar.close()\n",
        "                print(f'Epoch [{epoch + 1} / {num_epochs}] \\t Step [{step + 1} / {len(train_loader)}] \\t '\n",
        "                      f'Train Loss: {total_loss / print_every}')\n",
        "                total_loss = 0\n",
        "\n",
        "                pbar = tqdm(total=print_every, leave=False)\n",
        "\n",
        "        pbar.close()\n",
        "        val_loss = validate(valid_loader, model, criterion)\n",
        "        val_losses.append((total_step, val_loss))\n",
        "        if val_loss < lowest_val:\n",
        "            lowest_val = val_loss\n",
        "            torch.save(model, 'output/transformer.pth')\n",
        "        print(f'Val Loss: {val_loss}')\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def validate(valid_loader, model, criterion):\n",
        "    pbar = tqdm(total=len(iter(valid_loader)), leave=False)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    for src, src_key_padding_mask, tgt, tgt_key_padding_mask in iter(valid_loader):\n",
        "        with torch.no_grad():\n",
        "            src, src_key_padding_mask = src[0].to(device), src_key_padding_mask[0].to(device)\n",
        "            tgt, tgt_key_padding_mask = tgt[0].to(device), tgt_key_padding_mask[0].to(device)\n",
        "            memory_key_padding_mask = src_key_padding_mask.clone()\n",
        "            tgt_inp = tgt[:, :-1]\n",
        "            tgt_out = tgt[:, 1:].contiguous()\n",
        "            tgt_mask = gen_nopeek_mask(tgt_inp.shape[1]).to(device)\n",
        "\n",
        "            outputs = model(src, tgt_inp, src_key_padding_mask, tgt_key_padding_mask[:, :-1], memory_key_padding_mask, tgt_mask)\n",
        "            loss = criterion(rearrange(outputs, 'b t v -> (b t) v'), rearrange(tgt_out, 'b o -> (b o)'))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "    model.train()\n",
        "    return total_loss / len(valid_loader)\n",
        "\n",
        "\n",
        "def gen_nopeek_mask(length):\n",
        "    mask = rearrange(torch.triu(torch.ones(length, length)) == 1, 'h w -> w h')\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4aH0cGVFw52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 20\n",
        "max_seq_length = 96\n",
        "num_tokens = 2000\n",
        "\n",
        "vocab_size = 1000+4\n",
        "d_model = 512\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "dim_feedforward = 2048\n",
        "nhead = 8\n",
        "pos_dropout = 0.1\n",
        "trans_dropout = 0.1\n",
        "n_warmup_steps = 4000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmdUcQGvTSIV",
        "colab_type": "code",
        "outputId": "be8af3cb-907c-421c-fbe5-a164f37f5ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-6c06cbb43ed7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-6c06cbb43ed7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, optim, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b t v -> (b t) v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b o -> (b o)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1ce803fbfc05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask,\n\u001b[0;32m---> 26\u001b[0;31m                                   tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m't n e -> n t e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the feature number of src and tgt must be equal to d_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[1;32m    120\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             output = self.layers[i](output, src_mask=mask,\n\u001b[0;32m--> 176\u001b[0;31m                                     src_key_padding_mask=src_key_padding_mask)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \"\"\"\n\u001b[1;32m    282\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0;32m--> 283\u001b[0;31m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   3218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3220\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3221\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3222\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWgIK8FeEmLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_words(sentence, freq_list):\n",
        "    return [freq_list[word] for word in sentence if word in freq_list]\n",
        "\n",
        "def preprocess_data(path, vocab_folder, lang, split = 0.8):\n",
        "\n",
        "    punct = ['(', ')', ':', '\"', ' ']\n",
        "\n",
        "    data = []\n",
        "    with open(path, 'r') as fp:\n",
        "        for line in fp:\n",
        "            data.append(line.strip())\n",
        "\n",
        "    proc_data = []\n",
        "    for sentence in data:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = [elem for elem in sentence.split(\" \") if elem not in punct]\n",
        "        proc_data.append(sentence)\n",
        "        # sentence = [tok.text for tok in lang_model.tokenizer(sentence) if tok.text not in punctuation]\n",
        "    # lang_data = load_data(data_path)\n",
        "    # lang_model = spacy.load(lang, disable=['tagger', 'parser', 'ner'])\n",
        "\n",
        "    indices = [i for i in range(len(data))]\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    # 80:20:0 train validation test split\n",
        "    train_idx = int(len(data) * split)\n",
        "\n",
        "    train_indices = indices[:train_idx]\n",
        "    test_indices = indices[train_idx:]\n",
        "    # processed_sentences = [process_sentences(lang_model, sentence, punctuation) for sentence in lang_data]\n",
        "\n",
        "    train = [proc_data[i] for i in train_indices]\n",
        "\n",
        "    freq_list = Counter()\n",
        "    for sentence in train:\n",
        "        freq_list.update(sentence)\n",
        "    # freq_list = freq_list.most_common(10000)\n",
        "\n",
        "    # Map words in the dictionary to indices but reserve 0 for padding,\n",
        "    # 1 for out of vocabulary words, 2 for start-of-sentence and 3 for end-of-sentence\n",
        "    freq_list = {freq[0]: i + 4 for i, freq in enumerate(freq_list)}\n",
        "    freq_list['[PAD]'] = 0\n",
        "    freq_list['[OOV]'] = 1\n",
        "    freq_list['[SOS]'] = 2\n",
        "    freq_list['[EOS]'] = 3\n",
        "    proc_data = [map_words(sentence, freq_list) for sentence in tqdm(proc_data)]\n",
        "\n",
        "    train = [proc_data[i] for i in train_indices]\n",
        "    test = [proc_data[i] for i in test_indices]\n",
        "    # test = [processed_sentences[i] for i in test_indices]\n",
        "\n",
        "    os.makedirs(f'{vocab_folder}/processed/{lang}', exist_ok=True)\n",
        "    with open(f'{vocab_folder}/processed/{lang}/train.pkl', 'wb') as f:\n",
        "        pickle.dump(train, f)\n",
        "    with open(f'{vocab_folder}/processed/{lang}/val.pkl', 'wb') as f:\n",
        "        pickle.dump(test, f)\n",
        "    # with open(f'data/processed/{lang}/test.pkl', 'wb') as f:\n",
        "    #     pickle.dump(test, f)\n",
        "    with open(f'{vocab_folder}/processed/{lang}/freq_list.pkl', 'wb') as f:\n",
        "        pickle.dump(freq_list, f)\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path_source, data_path_target, num_tokens, max_seq_length):\n",
        "        self.num_tokens = num_tokens\n",
        "\n",
        "        with open(data_path_source, 'rb') as f:\n",
        "            self.data_1 = pickle.load(f)\n",
        "        with open(data_path_target, 'rb') as f:\n",
        "            self.data_2 = pickle.load(f)\n",
        "\n",
        "        self.data_lengths = {}\n",
        "        for i, (str_1, str_2) in enumerate(zip(self.data_1, self.data_2)):\n",
        "            if 0 < len(str_1) <= max_seq_length and 0 < len(str_2) <= max_seq_length - 2:\n",
        "                if (len(str_1), len(str_2)) in self.data_lengths:\n",
        "                    self.data_lengths[(len(str_1), len(str_2))].append(i)\n",
        "                else:\n",
        "                    self.data_lengths[(len(str_1), len(str_2))] = [i]\n",
        "\n",
        "        dl = self.data_lengths.copy()\n",
        "        for k, v in dl.items():\n",
        "            random.shuffle(v)\n",
        "\n",
        "        batches = []\n",
        "        prev_tokens_in_batch = 1e10\n",
        "        for k in sorted(dl):\n",
        "            v = dl[k]\n",
        "            total_tokens = (k[0] + k[1]) * len(v)\n",
        "\n",
        "            while total_tokens > 0:\n",
        "                tokens_in_batch = min(total_tokens, num_tokens) - min(total_tokens, num_tokens) % (k[0] + k[1])\n",
        "                sentences_in_batch = tokens_in_batch // (k[0] + k[1])\n",
        "\n",
        "                # Combine with previous batch?\n",
        "                if tokens_in_batch + prev_tokens_in_batch <= num_tokens:\n",
        "                    batches[-1].extend(v[:sentences_in_batch])\n",
        "                    prev_tokens_in_batch += tokens_in_batch\n",
        "                else:\n",
        "                    batches.append(v[:sentences_in_batch])\n",
        "                    prev_tokens_in_batch = tokens_in_batch\n",
        "                v = v[sentences_in_batch:]\n",
        "\n",
        "                total_tokens = (k[0] + k[1]) * len(v)\n",
        "        self.batches = batches\n",
        "        # self.batches = gen_batches(num_tokens, self.data_lengths)\n",
        "\n",
        "    def __proc_data_mask(self, idx, src = True):\n",
        "        sentence_indices = self.batches[idx]\n",
        "        if src:\n",
        "            batch = [self.data_1[i] for i in sentence_indices]\n",
        "        else:\n",
        "            batch = [[2] + self.data_2[i] + [3] for i in sentence_indices]\n",
        "\n",
        "        seq_length = 0\n",
        "        for sentence in batch:\n",
        "            if len(sentence) > seq_length:\n",
        "                seq_length = len(sentence)\n",
        "\n",
        "        masks = []\n",
        "        for i, sentence in enumerate(batch):\n",
        "            masks.append([False for _ in range(len(sentence))] + [True for _ in range(seq_length - len(sentence))])\n",
        "            batch[i] = sentence + [0 for _ in range(seq_length - len(sentence))]\n",
        "\n",
        "        return np.array(batch), np.array(masks)\n",
        "    def __getitem__(self, item):\n",
        "        src, src_mask = self.__proc_data_mask(item, True)\n",
        "        tgt, tgt_mask = self.__proc_data_mask(item, False)\n",
        "\n",
        "        return src, src_mask, tgt, tgt_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def shuffle_batches(self):\n",
        "\n",
        "        dl = self.data_lengths.copy()\n",
        "        for k, v in dl.items():\n",
        "            random.shuffle(v)\n",
        "\n",
        "        batches = []\n",
        "        prev_tokens_in_batch = 1e10\n",
        "        for k in sorted(dl):\n",
        "            v = dl[k]\n",
        "            total_tokens = (k[0] + k[1]) * len(v)\n",
        "\n",
        "            while total_tokens > 0:\n",
        "                tokens_in_batch = min(total_tokens, self.num_tokens) - min(total_tokens, self.num_tokens) % (k[0] + k[1])\n",
        "                sentences_in_batch = tokens_in_batch // (k[0] + k[1])\n",
        "\n",
        "                # Combine with previous batch?\n",
        "                if tokens_in_batch + prev_tokens_in_batch <= self.num_tokens:\n",
        "                    batches[-1].extend(v[:sentences_in_batch])\n",
        "                    prev_tokens_in_batch += tokens_in_batch\n",
        "                else:\n",
        "                    batches.append(v[:sentences_in_batch])\n",
        "                    prev_tokens_in_batch = tokens_in_batch\n",
        "                v = v[sentences_in_batch:]\n",
        "\n",
        "                total_tokens = (k[0] + k[1]) * len(v)\n",
        "        self.batches = batches\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BBr2S-JesM",
        "colab_type": "code",
        "outputId": "ed5a00a6-6711-45eb-bb5f-26b290105a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/andrewpeng02/transformer-translation.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformer-translation' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klCLLKW0NYIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r transformer-translation/* ./ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VglvW00NNhiL",
        "colab_type": "code",
        "outputId": "3592e3c9-4f8d-4434-ada2-4b05d55144a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "!python train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0% 0/500 [00:00<?, ?it/s]torch.Size([7, 142, 512]) torch.Size([8, 142, 512])\n",
            "  0% 1/500 [00:00<06:22,  1.30it/s]torch.Size([10, 95, 512]) torch.Size([12, 95, 512])\n",
            "  0% 2/500 [00:01<05:16,  1.57it/s]torch.Size([10, 95, 512]) torch.Size([12, 95, 512])\n",
            "  1% 3/500 [00:01<04:23,  1.89it/s]torch.Size([8, 117, 512]) torch.Size([10, 117, 512])\n",
            "  1% 4/500 [00:01<03:45,  2.20it/s]torch.Size([9, 111, 512]) torch.Size([10, 111, 512])\n",
            "  1% 5/500 [00:01<03:18,  2.50it/s]torch.Size([13, 74, 512]) torch.Size([15, 74, 512])\n",
            "  1% 6/500 [00:02<02:58,  2.77it/s]torch.Size([8, 76, 512]) torch.Size([16, 76, 512])\n",
            "  1% 7/500 [00:02<02:42,  3.03it/s]torch.Size([6, 166, 512]) torch.Size([7, 166, 512])\n",
            "  2% 8/500 [00:02<02:34,  3.19it/s]torch.Size([8, 133, 512]) torch.Size([8, 133, 512])\n",
            "  2% 9/500 [00:03<02:28,  3.30it/s]torch.Size([16, 22, 512]) torch.Size([19, 22, 512])\n",
            "  2% 10/500 [00:03<02:10,  3.75it/s]torch.Size([10, 100, 512]) torch.Size([11, 100, 512])\n",
            "  2% 11/500 [00:03<02:11,  3.71it/s]torch.Size([10, 100, 512]) torch.Size([11, 100, 512])\n",
            "  2% 12/500 [00:03<02:12,  3.69it/s]torch.Size([7, 111, 512]) torch.Size([12, 111, 512])\n",
            "  3% 13/500 [00:04<02:13,  3.64it/s]torch.Size([6, 133, 512]) torch.Size([10, 133, 512])\n",
            "  3% 14/500 [00:04<02:14,  3.60it/s]torch.Size([7, 142, 512]) torch.Size([8, 142, 512])\n",
            "  3% 15/500 [00:04<02:15,  3.58it/s]torch.Size([9, 111, 512]) torch.Size([10, 111, 512])\n",
            "  3% 16/500 [00:04<02:14,  3.59it/s]torch.Size([4, 250, 512]) torch.Size([5, 250, 512])\n",
            "  3% 17/500 [00:05<02:15,  3.56it/s]torch.Size([9, 105, 512]) torch.Size([11, 105, 512])\n",
            "  4% 18/500 [00:05<02:14,  3.58it/s]torch.Size([5, 200, 512]) torch.Size([6, 200, 512])\n",
            "  4% 19/500 [00:05<02:15,  3.54it/s]torch.Size([10, 86, 512]) torch.Size([14, 86, 512])\n",
            "  4% 20/500 [00:06<02:15,  3.54it/s]torch.Size([7, 166, 512]) torch.Size([6, 166, 512])\n",
            "  4% 21/500 [00:06<02:14,  3.56it/s]torch.Size([8, 117, 512]) torch.Size([10, 117, 512])\n",
            "  4% 22/500 [00:06<02:13,  3.58it/s]torch.Size([6, 166, 512]) torch.Size([7, 166, 512])\n",
            "  5% 23/500 [00:06<02:13,  3.57it/s]torch.Size([6, 153, 512]) torch.Size([8, 153, 512])\n",
            "  5% 24/500 [00:07<02:13,  3.57it/s]torch.Size([6, 153, 512]) torch.Size([8, 153, 512])\n",
            "  5% 25/500 [00:07<02:12,  3.58it/s]torch.Size([4, 222, 512]) torch.Size([6, 222, 512])\n",
            "\n",
            "Aborted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gWePn4YWgY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}